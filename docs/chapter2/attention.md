# ğŸŒŸ æ³¨æ„åŠ›æœºåˆ¶æ€»è§ˆï¼ˆAttention Mechanisms Overviewï¼‰

---

## 1. åŸå§‹æ³¨æ„åŠ›æœºåˆ¶ï¼ˆScaled Dot-Product Attentionï¼‰

### ğŸ§  æ ¸å¿ƒæ€æƒ³
é€šè¿‡è®¡ç®— Query ä¸ Key çš„ç‚¹ç§¯ç›¸ä¼¼åº¦ï¼Œå¾—åˆ°æ¯ä¸ªä½ç½®å¯¹åºåˆ—ä¸­å…¶ä»–ä½ç½®çš„æ³¨æ„åŠ›æƒé‡ï¼Œå¹¶ç”¨è¯¥æƒé‡åŠ æƒ Valueï¼Œå¾—åˆ°ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚

### ğŸ§© å…¬å¼
$$
\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$



### ğŸ” æ­¥éª¤
1. å¯¹è¾“å…¥å‘é‡ $q, k, v$ åˆ†åˆ«æ˜ å°„ä¸º Q, K, Vï¼›
2. è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†çŸ©é˜µï¼›
3. ç»è¿‡ softmax å½’ä¸€åŒ–ï¼›
4. åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡ºã€‚

### ğŸ’¡ ç‰¹ç‚¹
- å…¨å±€æ³¨æ„åŠ›ï¼Œå…¨åºåˆ—äº¤äº’ï¼›
- å¤æ‚åº¦ $O(n^2)$ï¼Œ$n$ä¸ºåºåˆ—çš„é•¿åº¦ï¼›
- é«˜æ€§èƒ½ä½†éš¾ä»¥æ‰©å±•åˆ°é•¿åºåˆ—ã€‚

## 2. å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Head Attention, MHAï¼‰

### ğŸ§  æ ¸å¿ƒæ€æƒ³
å°†æ³¨æ„åŠ›åˆ†æˆå¤šä¸ªå¤´ï¼ˆheadï¼‰ï¼Œè®©æ¨¡å‹å­¦ä¹ ä¸åŒçš„è¯­ä¹‰å­ç©ºé—´è¡¨ç¤ºã€‚

### ğŸ§© å…¬å¼
$$
\text{MHA}(Q,K,V) = \text{Concat}(\text{head}_1, \dots, \text{head}_h) W^O
$$

å…¶ä¸­
$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$


### ğŸ’¡ ç‰¹ç‚¹
- å¤šå¤´æ•è·ä¸åŒå…³ç³»æ¨¡å¼ï¼›
- å¹¶è¡Œæ€§å¼ºï¼›
- ç¼ºç‚¹ï¼šæ¨ç†é˜¶æ®µæ¯ä¸ª head éƒ½è¦ç»´æŠ¤ç‹¬ç«‹çš„ KV ç¼“å­˜ï¼Œæ˜¾å­˜å¼€é”€å¤§ã€‚

```python
class MHA(nn.Module):
    """
    Multi-Head Attention Module With KV-Cache
    
    Args:
        args: ModelArgs
            model arguments
    """
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.dim = args.dim
        self.num_head = args.num_head
        assert self.dim % self.num_head == 0
        self.dim_head = self.dim // self.num_head
        self.dropout = args.dropout
        self.max_seq_len = args.max_seq_len
        
        self.W_q = nn.Linear(self.dim, self.dim, bias=False)
        self.W_k = nn.Linear(self.dim, self.dim, bias=False)
        self.W_v = nn.Linear(self.dim, self.dim, bias=False)
        self.atten_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.W_o = nn.Linear(self.dim, self.dim, bias=False)
        
    def forward(
        self, 
        q: Tensor, 
        k: Tensor, 
        v: Tensor, 
        mask: Optional[Tensor] = None,
        kv_cache: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Optional[Tuple[Tensor, Tensor]]]: # return (out, kv_cache)
        """
        q: (B, S_q, dim)
            S_q may be 1 (in inference) or T (in training)
        k: (B, S_k, dim)
        v: (B, S_v, dim)
            S_k and S_v may be 1 (in inference) or T (in training)
            S_k and S_v must be equal, but may differ from S_q
        mask: (B, 1, S_q, S_k)
        kv_cache: ( (B, H, S_past_k, dim_head), (B, H, S_past_v, dim_head) )
        """
        B = q.size(0)
        Q: Tensor = self.W_q(q) 
        K: Tensor = self.W_k(k)
        V: Tensor = self.W_v(v)

        # Reshape (B, S, dim) -> (B, H, S, dim_head), H * dim_head = dim
        Q = Q.reshape(B, -1, self.num_head, self.dim_head).permute(0, 2, 1, 3)
        K_new = K.reshape(B, -1, self.num_head, self.dim_head).permute(0, 2, 1, 3)
        V_new = V.reshape(B, -1, self.num_head, self.dim_head).permute(0, 2, 1, 3)

        if kv_cache is not None:
            # In inference mode
            # past_K/V: (B, H, S_past, dim_head)
            past_K, past_V = kv_cache
            # (B, H, S_past, dim_head) + (B, H, S_k, dim_head) -> (B, H, S_past + S_k, dim_head)
            K = torch.cat([past_K, K_new], dim=2)
            V = torch.cat([past_V, V_new], dim=2)
            # new cache
            kv_cache = (K, V)
        else:
            # In training mode
            K = K_new
            V = V_new

        # Q @ K.T -> (B, H, S_q, S_k_total)
        # Inference: (S_q=1, S_k_total=i): (B,H,1,dim) @ (B,H,dim,i) -> (B,H,1,i)
        # Training: (S_q=T, S_k_total=T): (B,H,T,dim) @ (B,H,dim,T) -> (B,H,T,T)
        attention = Q @ K.transpose(-1, -2) / math.sqrt(self.dim_head)
        if mask is not None:
            attention = attention.masked_fill(mask == 0, float('-inf'))
        attention = torch.softmax(attention.float(), -1).to(q.dtype)
        attention: Tensor = self.atten_dropout(attention)
        # Inference: (B,H,1,i) @ (B,H,i,dim) -> (B,H,1,dim)
        # Training: (B,H,T,T) @ (B,H,T,dim) -> (B,H,T,dim)
        out = attention @ V
        
        # (B, H, S_q, dim_head) -> (B, S_q, H, dim_head) -> (B, S_q, dim)
        out = out.permute(0, 2, 1, 3).reshape(B, -1, self.dim)
        out = self.resid_dropout(self.W_o(out))
        
        return out, kv_cache
```

å¦‚ä¸Šè¿°ä»£ç æ‰€ç¤ºï¼Œç»´æŠ¤çš„kv cacheç»“æ„æ˜¯ $( (B, H, S_{past_k}, dim_{head}), (B, H, S_{past_v}, dim_{head}) )$ï¼Œå³å¯¹äºæ¯ä¸€ä¸ªheadæ¥è¯´ï¼Œéƒ½éœ€è¦ç»´æŠ¤ä¹‹å‰æ‰€æœ‰tokenå¯¹åº”çš„Kå’ŒVï¼Œæ˜¾å­˜å¼€é”€æå¤§ã€‚

## 3. å•æŸ¥è¯¢å¤šå¤´æ³¨æ„åŠ›ï¼ˆMulti-Query Attention, MQAï¼‰

### ğŸ§  æ ¸å¿ƒæ€æƒ³
æ‰€æœ‰ Query å¤´å…±äº«ä¸€ç»„ Key/Value æŠ•å½±ï¼Œç”¨äºåŠ é€Ÿæ¨ç†ã€‚

### ğŸ§© å…¬å¼
$$
Q_i = XW_i^Q,\quad K = XW^K,\quad V = XW^V
$$

### ğŸ’¡ ä¼˜ç‚¹
- æ˜¾å­˜èŠ‚çœï¼šKV ç¼“å­˜åªå­˜ä¸€ä»½ï¼›
- æ¨ç†é€Ÿåº¦æ˜¾è‘—æé«˜ï¼›
- å‚æ•°é‡ä¸ MHA å‡ ä¹ç›¸åŒã€‚

### âš ï¸ ç¼ºç‚¹
- ä¸åŒ head å…±ç”¨åŒä¸€ K/Vï¼Œè¡¨è¾¾èƒ½åŠ›é™ä½ï¼›
- æ¨¡å‹å¤šæ ·æ€§ä¸‹é™ã€‚

### ğŸ”— åº”ç”¨
- Google PaLM
- Efficient Transformers

```python
class MQA(nn.Module):
    """
    Multi-Query Attention Module With KV-Cache
    
    Args:
        args: ModelArgs
            model arguments
    """
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.dim = args.dim
        self.num_head = args.num_head
        assert self.dim % self.num_head == 0
        self.dim_head = self.dim // self.num_head
        self.dropout = args.dropout
        self.max_seq_len = args.max_seq_len
        
        self.W_q = nn.Linear(self.dim, self.dim, bias=False)
        # k/v share the same projection in MQA
        # Thus, the number of k/v projection matrices is 1 instead of num_head
        self.W_k = nn.Linear(self.dim, self.dim_head, bias=False)
        self.W_v = nn.Linear(self.dim, self.dim_head, bias=False)
        self.atten_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.W_o = nn.Linear(self.dim, self.dim, bias=False)
        
    def forward(
        self, 
        q: Tensor, 
        k: Tensor, 
        v: Tensor, 
        mask: Optional[Tensor] = None,
        kv_cache: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Optional[Tuple[Tensor, Tensor]]]: # return (out, kv_cache)
        """
        q: (B, S_q, dim)
            S_q may be 1 (in inference) or T (in training)
        k: (B, S_k, dim)
        v: (B, S_v, dim)
            S_k and S_v may be 1 (in inference) or T (in training)
            S_k and S_v must be equal, but may differ from S_q
        mask: (B, 1, S_q, S_k)
        kv_cache: ( (B, S_past_k, dim_head), (B, S_past_v, dim_head) )
        """
        B = q.size(0)
        Q: Tensor = self.W_q(q) 
        K: Tensor = self.W_k(k)
        V: Tensor = self.W_v(v)

        # Reshape (B, S, dim) -> (B, H, S, dim_head)
        Q = Q.reshape(B, -1, self.num_head, self.dim_head).permute(0, 2, 1, 3)
        # (B, S, dim_head)
        K_new, V_new = K, V
        # (B, 1, S, dim_head)
        K = K.reshape(B, -1, 1, self.dim_head).permute(0, 2, 1, 3)
        V = V.reshape(B, -1, 1, self.dim_head).permute(0, 2, 1, 3)

        if kv_cache is not None:
            # In inference mode
            past_K, past_V = kv_cache
            K_new = torch.cat([past_K, K_new], dim=1)
            V_new = torch.cat([past_V, V_new], dim=1)
        kv_cache = (K_new, V_new)

        attention = Q @ K.transpose(-1, -2) / math.sqrt(self.dim_head)
        if mask is not None:
            attention = attention.masked_fill(mask == 0, float('-inf'))
        attention = torch.softmax(attention.float(), -1).to(q.dtype)
        attention: Tensor = self.atten_dropout(attention)
        out = attention @ V

        out = out.permute(0, 2, 1, 3).reshape(B, -1, self.dim)
        out = self.resid_dropout(self.W_o(out))
        
        return out, kv_cache
```

å¦‚ä¸Šè¿°ä»£ç æ‰€ç¤ºï¼Œkv cacheçš„å½¢çŠ¶æ˜¯( (B, S_past_k, dim_head), (B, S_past_v, dim_head) )ï¼Œä¸Hæ— å…³ã€‚

ç›¸å½“äºæœ‰Hä¸ªheadï¼Œä½†åªéœ€è¦ä¿å­˜ä¸€ä»½KVã€‚

## 4. åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGrouped-Query Attention, GQAï¼‰

### ğŸ§  æ ¸å¿ƒæ€æƒ³
ä»‹äº MHA ä¸ MQA ä¹‹é—´çš„æŠ˜ä¸­ï¼š
å¤šä¸ª Query head å…±äº«åŒä¸€ç»„ K/Vï¼ˆæŒ‰ç»„å…±äº«ï¼‰ã€‚

### ğŸ§© å…¬å¼
$$
Q_i = XW_i^Q, \quad (K_i, V_i) = XW_j^K, XW_j^V, \quad i \in \text{group}_j
$$



### ğŸ’¡ ç‰¹ç‚¹

-   **å†…å­˜æ•ˆç‡**ï¼šKV ç¼“å­˜æ•°é‡ä¸ºç»„æ•° $g$ï¼›

-   **çµæ´»æ€§é«˜**ï¼šå¯è°ƒèŠ‚ç»„æ•° $g$ æ§åˆ¶æ€§èƒ½ä¸æ˜¾å­˜ä¹‹é—´çš„æŠ˜ä¸­ï¼›

-   **å…¼å®¹æ€§å¼º**ï¼šå‡ ä¹å¯ç›´æ¥æ›¿æ¢ MHA å±‚ã€‚

### ğŸ”— åº”ç”¨

- LLaMA 2 / 3
- Mistral
- Gemma
- Falcon 180B

```python
class GQA(nn.Module):
    """
    Group-Query Attention Module With KV-Cache
    
    Args:
        args: ModelArgs
            model arguments
    """
    def __init__(self, args: ModelArgs):
        super().__init__()
        self.dim = args.dim
        self.num_head = args.num_head
        self.num_group = args.num_group
        assert self.num_group <= self.num_head and self.num_head % self.num_group == 0
        assert self.dim % self.num_head == 0
        self.dim_head = self.dim // self.num_head
        self.dropout = args.dropout
        self.max_seq_len = args.max_seq_len
        
        self.W_q = nn.Linear(self.dim, self.dim, bias=False)
        # k/v share the same projection in MQA
        # Thus, the number of k/v projection matrices is num_gruop instead of num_head
        self.W_k = nn.Linear(self.dim, self.num_group * self.dim_head, bias=False)
        self.W_v = nn.Linear(self.dim, self.num_group * self.dim_head, bias=False)
        self.atten_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.W_o = nn.Linear(self.dim, self.dim, bias=False)
 
        
    def forward(
        self, 
        q: Tensor, 
        k: Tensor, 
        v: Tensor, 
        mask: Optional[Tensor] = None,
        kv_cache: Optional[Tuple[Tensor, Tensor]] = None
    ) -> Tuple[Tensor, Optional[Tuple[Tensor, Tensor]]]: # return (out, kv_cache)
        """
        q: (B, S_q, dim)
            S_q may be 1 (in inference) or T (in training)
        k: (B, S_k, dim)
        v: (B, S_v, dim)
            S_k and S_v may be 1 (in inference) or T (in training)
            S_k and S_v must be equal, but may differ from S_q
        mask: (B, 1, S_q, S_k)
        kv_cache: ( (B, num_group, S_past_k, dim_head), (B, num_group, S_past_v, dim_head) )
        """
        B = q.size(0)
        Q: Tensor = self.W_q(q) 
        K: Tensor = self.W_k(k)
        V: Tensor = self.W_v(v)

        # Reshape (B, S, dim) -> (B, H, S, dim_head)
        Q = Q.reshape(B, -1, self.num_head, self.dim_head).permute(0, 2, 1, 3)
        # (B, num_group, S, dim_head)
        K_new = K.reshape(B, -1, self.num_group, self.dim_head).permute(0, 2, 1, 3)
        V_new = V.reshape(B, -1, self.num_group, self.dim_head).permute(0, 2, 1, 3)

        if kv_cache is not None:
            # In inference mode
            past_K, past_V = kv_cache
            # (B, num_group, S_past, dim_head) + (B, num_group, S_k, dim_head) 
            #       -> (B, num_group, S_past + S_k, dim_head)
            K = torch.cat([past_K, K_new], dim=2)
            V = torch.cat([past_V, V_new], dim=2)
        else:
            # In training mode
            K = K_new
            V = V_new
        kv_cache = (K, V)

        # expand K/V to (B, H, S, dim_head)
        K = K.repeat_interleave(self.num_head // self.num_group, dim=1)
        V = V.repeat_interleave(self.num_head // self.num_group, dim=1)

        attention = Q @ K.transpose(-1, -2) / math.sqrt(self.dim_head)
        if mask is not None:
            attention = attention.masked_fill(mask == 0, float('-inf'))
        attention = torch.softmax(attention.float(), -1).to(q.dtype)
        attention: Tensor = self.atten_dropout(attention)
        out = attention @ V

        out = out.permute(0, 2, 1, 3).reshape(B, -1, self.dim)
        out = self.resid_dropout(self.W_o(out))
        
        return out, kv_cache
```

ä»£ç ä¸­ä¸»è¦æ˜¯kv cahceéƒ¨åˆ†ï¼Œåªéœ€è¦å­˜å‚¨num_groupä¸ªKVå®ä¾‹ï¼Œä¸€èˆ¬num_groupä¸º4,8,16ã€‚

å¹¶ä¸”å½“num_group = num_headæ—¶ï¼Œå°±ç­‰ä»·äºMHAï¼›

å½“num_group = 1æ—¶ï¼Œå°±ç­‰ä»·äºMQAã€‚

## MHA MQA GQA MLA å¯¹æ¯”

![image-20251111145416222](pictures/attention/image-20251111145416222.png)

å¦‚å›¾æ‰€ç¤ºï¼Œå‰ä¸‰ç§æ–¹æ³•çš„æ ¸å¿ƒåŒºåˆ«åœ¨äº**æŸ¥è¯¢å¤´ (Q) ä¸é”®/å€¼å¤´ (K/V) ä¹‹é—´çš„å…³ç³»**ï¼Œè¿™ç›´æ¥å†³å®šäº†æ¨ç†æ—¶ KV Cache çš„å¤§å°ï¼š

-   **MHA (å¤šå¤´æ³¨æ„åŠ›):** è¿™æ˜¯æ ‡å‡†é…ç½®ã€‚å¦‚å›¾æ‰€ç¤ºï¼Œæ¯ä¸€ä¸ª Q å¤´éƒ½**ä¸€å¯¹ä¸€**åŒ¹é…ä¸€ç»„ç‹¬ç«‹çš„ K/V å¤´ã€‚å¦‚æœæœ‰ 8 ä¸ª Q å¤´ï¼Œå°±æœ‰ 8 ç»„ K/Vã€‚è¿™æä¾›äº†æœ€å¼ºçš„æ¨¡å‹è¡¨è¾¾èƒ½åŠ›ï¼Œä½†ä»£ä»·æ˜¯ **KV ç¼“å­˜æœ€å¤§**ã€‚
-   **MQA (å¤šæŸ¥è¯¢æ³¨æ„åŠ›):** è¿™æ˜¯ä¸€ç§æ¿€è¿›çš„ä¼˜åŒ–ã€‚æ‰€æœ‰çš„ Q å¤´**å…±äº«åŒä¸€ç»„ K/V å¤´**ã€‚è¿™ä½¿å¾— KV ç¼“å­˜é™åˆ°äº†æœ€ä½ï¼ˆä»… 1 ç»„ï¼‰ï¼Œæå¤§æå‡äº†æ¨ç†é€Ÿåº¦å’Œååé‡ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€äº›æ¨¡å‹ç²¾åº¦ã€‚
-   **GQA (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›):** è¿™æ˜¯ MHA å’Œ MQA ä¹‹é—´çš„å®Œç¾å¹³è¡¡ã€‚å®ƒå°† Q å¤´åˆ†ç»„ï¼ˆå¦‚å›¾ä¸­ 2 ä¸ª Q ä¸ºä¸€ç»„ï¼‰ï¼Œ**ç»„å†…å…±äº«**ä¸€ç»„ K/V å¤´ã€‚è¿™åœ¨å®ç°æ¥è¿‘ MHA è´¨é‡çš„åŒæ—¶ï¼Œå°† KV ç¼“å­˜æ˜¾è‘—å‡å°ï¼ˆä¾‹å¦‚ï¼Œä» 8 ç»„é™åˆ° 4 ç»„ï¼‰ï¼Œæ˜¯ç›®å‰ï¼ˆå¦‚ Llama 2/3ï¼‰çš„ä¸»æµé€‰æ‹©ã€‚

ç¬¬å››ç§ï¼šå¤šå¤´æ½œåœ¨æ³¨æ„åŠ› (Multi-Head Latent Attention, MLA)ï¼Œä¸å‰ä¸‰ç§æ–¹æ³•é€šè¿‡ *å‡å°‘ K/V å¤´çš„æ•°é‡* æ¥èŠ‚çœæ˜¾å­˜çš„æ€è·¯ä¸åŒï¼ŒMLA æå‡ºäº†ä¸€ç§å…¨æ–°çš„ä¼˜åŒ–å“²å­¦ï¼š**å‹ç¼© K/V ç¼“å­˜çš„è¡¨ç¤º**ã€‚

MLA å‡è®¾ï¼Œç”± MHA ç”Ÿæˆçš„å®Œæ•´ K/V çŠ¶æ€æ˜¯é«˜åº¦å†—ä½™çš„ã€‚æˆ‘ä»¬ä¸éœ€è¦é€šè¿‡â€œå…±äº«â€æˆ–â€œåˆ†ç»„â€æ¥å‡å°‘å¤´çš„æ•°é‡ï¼Œè€Œæ˜¯å¯ä»¥å­¦ä¹ ä¸€ä¸ªæ›´å°ã€ä¿¡æ¯å¯†åº¦æ›´é«˜çš„â€œ**æ½œåœ¨è¡¨ç¤º**â€ï¼ˆLatent Representationï¼‰æ¥ä½œä¸º K/V ç¼“å­˜çš„â€œæ‘˜è¦â€ã€‚

ä»å›¾ç¤ºä¸­å¯ä»¥æ¸…æ™°åœ°çœ‹åˆ°ï¼š

1.  æ¨¡å‹åœ¨å†…éƒ¨ä¾ç„¶è®¡ç®—å‡ºäº†å®Œæ•´çš„ã€ä¸ MHA ç›¸åŒçš„**å¤šç»„ K/V å¤´**ï¼ˆå›¾ä¸­æœ‰ 8 ç»„ï¼‰ã€‚
2.  åœ¨å°†å®ƒä»¬å†™å…¥ç¼“å­˜ä¹‹å‰ï¼Œè¿™äº›å®Œæ•´çš„ K/V å¤´ä¼šç»è¿‡ä¸€ä¸ªâ€œ**æŠ•å½± (projection)**â€æ­¥éª¤ã€‚
3.  è¿™ä¸ªæŠ•å½±æ­¥éª¤ä¼šç”Ÿæˆä¸€ä¸ªâ€œ**å‹ç¼©çš„æ½œåœ¨ KV (Compressed Latent KV)**â€ã€‚
4.  **åªæœ‰è¿™ä¸ªå‹ç¼©åçš„è¡¨ç¤ºæ‰ä¼šè¢«çœŸæ­£ç¼“å­˜èµ·æ¥**ï¼Œç”¨äºåç»­çš„æ¨ç†æ­¥éª¤ã€‚

è¿™ç§æ–¹æ³•çš„å®ç°é€šå¸¸æ¶‰åŠä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š

-   **æŠ•å½± (Projection):** è¿™ä¸ªâ€œæŠ•å½±â€å±‚é€šå¸¸æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°çŸ©é˜µï¼ˆä¾‹å¦‚ä¸€ä¸ªç®€å•çš„çº¿æ€§å±‚æˆ–å°å‹å‰é¦ˆç½‘ç»œï¼‰ã€‚å®ƒçš„ä½œç”¨æ˜¯**é™ä½ç»´åº¦**ã€‚ä¾‹å¦‚ï¼Œå®ƒå¯ä»¥å°† 8 ä¸ª K/V å¤´çš„è¡¨ç¤ºâ€œæµ“ç¼©â€æˆ 2 ä¸ªâ€œæ½œåœ¨â€ K/V å¤´çš„è¡¨ç¤ºï¼Œæˆ–è€…ä¿æŒå¤´çš„æ•°é‡ä¸å˜ï¼Œä½†å‡å°æ¯ä¸ªå¤´çš„ç‰¹å¾ç»´åº¦ã€‚
-   **ç¼“å­˜ (Caching):** åœ¨æ¨ç†çš„æ¯ä¸€æ­¥ï¼Œæ¨¡å‹è®¡ç®—å‡º `N` ç»„ K/V çŠ¶æ€ï¼Œé€šè¿‡è¿™ä¸ªæŠ•å½±å±‚ï¼Œå¾—åˆ°ä¸€ä¸ªæ›´å°çš„ `L` ç»„ï¼ˆæˆ–ç»´åº¦æ›´ä½ï¼‰çš„æ½œåœ¨ K/V çŠ¶æ€ã€‚è¿™ä¸ªæ½œåœ¨çŠ¶æ€è¢«å†™å…¥ç¼“å­˜ï¼Œä»è€ŒèŠ‚çœäº†å¤§é‡ VRAMã€‚

MLA çš„ä¼˜åŠ¿åœ¨äºï¼Œå®ƒç†è®ºä¸Šä¿ç•™äº†æ‰€æœ‰ MHA åŸå§‹ K/V å¤´çš„ä¿¡æ¯ï¼ˆé€šè¿‡å­¦ä¹ å¦‚ä½•å‹ç¼©å®ƒä»¬ï¼‰ï¼Œè€Œä¸æ˜¯åƒ GQA æˆ– MQA é‚£æ ·åœ¨ç»“æ„ä¸Šå°±å‡å°‘äº† K/V å¤´çš„æ•°é‡ã€‚

æœ€åï¼Œæˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªè¡¨æ ¼æ¥æ¸…æ™°åœ°æ€»ç»“è¿™å››ç§æ–¹æ³•åœ¨æ¨ç†æ—¶å¯¹ KV ç¼“å­˜çš„å¤„ç†ï¼š

| **æ–¹æ³•** | **K/V å¤´é…ç½®**               | **KV ç¼“å­˜å¤§å°** | **æ ¸å¿ƒæ€æƒ³**                                |
| -------- | ---------------------------- | --------------- | ------------------------------------------- |
| **MHA**  | `N` ç»„ Q, `N` ç»„ K/V         | æœ€å¤§ (N å€)     | **ä¸ä¼˜åŒ–**ï¼šæ¯ä¸ª Q å¤´æœ‰ç‹¬ç«‹çš„ K/V           |
| **MQA**  | `N` ç»„ Q, `1` ç»„ K/V         | æœ€å° (1 å€)     | **å…¨å±€å…±äº«**ï¼šæ‰€æœ‰ Q å¤´å…±äº« 1 ç»„ K/V        |
| **GQA**  | `N` ç»„ Q, `G` ç»„ K/V         | ä¸­ç­‰ (G å€)     | **åˆ†ç»„å…±äº«**ï¼šæ¯ç»„ Q å¤´å…±äº« 1 ç»„ K/V        |
| **MLA**  | `N` ç»„ Q, `N` ç»„ K/V -> å‹ç¼© | å° (L å€)       | **å‹ç¼©**ï¼šç¼“å­˜ K/V çŠ¶æ€çš„ä¸€ä¸ªä½ç»´â€œæ½œåœ¨â€æŠ•å½± |

## 5. FlashAttentionï¼ˆé«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—ï¼‰

å‰é¢æˆ‘ä»¬è®¨è®ºçš„ MQA, GQA ç­‰æ–¹æ³•ï¼Œä¸»è¦ä¼˜åŒ–çš„æ˜¯æ¨ç†æ—¶ **KV Cache çš„æ˜¾å­˜å ç”¨**ã€‚è€Œ FlashAttention åˆ™ç€çœ¼äºä¸€ä¸ªæ›´æ ¹æœ¬çš„é—®é¢˜ï¼š**æ³¨æ„åŠ›è®¡ç®—æœ¬èº«çš„é€Ÿåº¦å’Œæ˜¾å­˜ç“¶é¢ˆ**ã€‚

å®ƒæ˜¯ä¸€ç§ I/O æ„ŸçŸ¥ï¼ˆI/O-awareï¼‰çš„æ³¨æ„åŠ›ç®—æ³•ï¼Œä¸ä¿®æ”¹æ³¨æ„åŠ›è®¡ç®—çš„æ•°å­¦æœ¬è´¨ï¼Œä½†é€šè¿‡å·§å¦™çš„å·¥ç¨‹å®ç°ï¼Œä½¿å…¶åœ¨ GPU ä¸Šçš„è¿è¡Œ**é€Ÿåº¦æ›´å¿«ã€æ˜¾å­˜å ç”¨æ›´ä½**ã€‚

### 1. æ ¸å¿ƒç“¶é¢ˆï¼šå†…å­˜å¸¦å®½ (The I/O Bottleneck)

æˆ‘ä»¬å…ˆå›é¡¾æ ‡å‡†æ³¨æ„åŠ›çš„è®¡ç®—ï¼š$O = \text{Softmax}(QK^T)V$ã€‚

åœ¨ GPU ä¸Šæ‰§è¡Œæ­¤æ“ä½œçš„æ ‡å‡†æ–¹æ³•ï¼ˆä¾‹å¦‚ PyTorch çš„ `nn.MultiHeadAttention`ï¼‰å­˜åœ¨ä¸€ä¸ªå·¨å¤§ç“¶é¢ˆï¼š

1.  è®¡ç®— $S = QK^T$ï¼š å‡è®¾åºåˆ—é•¿åº¦ä¸º `N`ï¼Œç‰¹å¾ç»´åº¦ä¸º `d`ã€‚è¿™ä¼šäº§ç”Ÿä¸€ä¸ªå·¨å¤§çš„ä¸­é—´çŸ©é˜µ $S$ï¼Œå¤§å°ä¸º $N \times N$ã€‚
2.  HRAM è¯»å†™ï¼š è¿™ä¸ª $N \times N$ çš„çŸ©é˜µ $S$ å¿…é¡»è¢«å†™å…¥ GPU çš„ **HRAM**ï¼ˆé«˜å¸¦å®½å†…å­˜ï¼Œå³ VRAM æ˜¾å­˜ï¼‰ï¼Œç„¶åå†ç”± Softmax æ“ä½œè¯»å‡ºã€‚
3.  Softmax æ“ä½œï¼š è®¡ç®— $\text{Softmax}(S)$ï¼Œç»“æœå†æ¬¡å†™å…¥ HRAMã€‚
4.  è®¡ç®— $O = \text{Softmax}(S)V$ï¼š å†æ¬¡ä» HRAM ä¸­è¯»å– $\text{Softmax}(S)$ï¼Œä¸ $V$ ç›¸ä¹˜ï¼Œæœ€åå°†ç»“æœ $O$ å†™å…¥ HRAMã€‚

é—®é¢˜åœ¨äºï¼š

-   **HRAM å¾ˆæ…¢ï¼š** ç›¸æ¯”äº GPU æ ¸å¿ƒçš„ç‰‡ä¸Š **SRAM**ï¼ˆè¶…é«˜é€Ÿç¼“å­˜ï¼‰ï¼ŒHRAM çš„è¯»å†™é€Ÿåº¦è¦æ…¢å‡ ä¸ªæ•°é‡çº§ã€‚
-   **I/O æˆä¸ºç“¶é¢ˆï¼š** æ³¨æ„åŠ›è®¡ç®—å˜æˆäº†**å†…å­˜å¸¦å®½é™åˆ¶ (memory-bound)** çš„æ“ä½œã€‚GPU æ ¸å¿ƒï¼ˆALUï¼‰å¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨â€œç­‰å¾…â€æ•°æ®ä» HRAM ä¼ æ¥æˆ–å†™å…¥ HRAMï¼Œè€Œä¸æ˜¯åœ¨çœŸæ­£åœ°â€œè®¡ç®—â€ï¼ˆFLOPsï¼‰ã€‚
-   **æ˜¾å­˜å ç”¨ï¼š** å½“åºåˆ—é•¿åº¦ `N` å¾ˆé•¿æ—¶ï¼ˆä¾‹å¦‚ 8Kã€16Kï¼‰ï¼Œè¿™ä¸ª $N \times N$ çš„ $S$ çŸ©é˜µæœ¬èº«å°±ä¼š**å ç”¨å·¨é‡çš„æ˜¾å­˜**ï¼ˆä¾‹å¦‚ï¼Œ`N=8K` æ—¶ï¼Œä¸€ä¸ª `float32` çŸ©é˜µ $S$ å°±éœ€è¦ $8192 \times 8192 \times 4 \text{ bytes} \approx 256 \text{ MB}$ï¼Œå¦‚æœæœ‰ 32 ä¸ªå¤´ï¼Œå°±æ˜¯ $8 \text{ GB}$ï¼Œè¿™è¿˜åªæ˜¯ä¸€ä¸ªå±‚ï¼ï¼‰ã€‚

### 2. FlashAttention çš„æ ¸å¿ƒåŸç†

FlashAttention çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼š**å½»åº•é¿å…å°† $N \times N$ çš„æ³¨æ„åŠ›çŸ©é˜µ $S$ å†™å…¥ HRAMã€‚**

å®ƒé€šè¿‡ä¸¤å¤§æŠ€æœ¯å®ç°äº†è¿™ä¸€ç‚¹ï¼š

1.  Kernel Fusion (æ ¸å‡½æ•°èåˆ):

    FlashAttention å°†æ ‡å‡†æ³¨æ„åŠ›ä¸­çš„å¤šæ¬¡ HRAM è¯»å†™ï¼ˆ$QK^T$ã€Softmaxã€$OV$ï¼‰èåˆæˆä¸€ä¸ªå•ç‹¬çš„ GPU Kernelï¼ˆæ ¸å‡½æ•°ï¼‰ã€‚

    è¿™ä¸ª Kernel ä¸€æ¬¡æ€§ä» HRAM ä¸­è¯»å– $Q$, $K$, $V$ï¼Œç„¶åå°†æ‰€æœ‰ä¸­é—´è®¡ç®—ï¼ˆ$S$ çŸ©é˜µã€Softmaxï¼‰å…¨éƒ¨åœ¨é«˜é€Ÿçš„ SRAM ä¸­å®Œæˆï¼Œæœ€ååªå°†æœ€ç»ˆçš„è¾“å‡º $O$ å†™å›åˆ° HRAMã€‚

2.  Tiling (åˆ†å—/ç“¦ç‰‡):

    SRAM çš„å®¹é‡éå¸¸å°ï¼ˆä¾‹å¦‚ï¼ŒNVIDIA A100 åªæœ‰ 192KB/SMï¼‰ï¼Œæ— æ³•å®¹çº³æ•´ä¸ª $N \times N$ çš„çŸ©é˜µã€‚

    FlashAttention ä½¿ç”¨åˆ†å—ç­–ç•¥ï¼šå®ƒå°† $Q$, $K$, $V$ çŸ©é˜µåˆ†å‰²æˆæ›´å°çš„å—ï¼ˆTilesï¼‰ã€‚Kernel åŠ è½½ $Q$ çš„ä¸€ä¸ªå—ï¼Œç„¶åè¿­ä»£åœ°åŠ è½½ $K$ å’Œ $V$ çš„ç›¸åº”å—ã€‚

    >   ä¸€ä¸ªå½¢è±¡çš„æ¯”å–»ï¼š
    >
    >   -   æ ‡å‡†æ³¨æ„åŠ›ï¼šå°±åƒä¸€ä¸ªå¨å¸ˆï¼ˆGPU æ ¸å¿ƒï¼‰ã€‚åšä¸€é“èœï¼ˆæ³¨æ„åŠ›è®¡ç®—ï¼‰ï¼Œæ¯åˆ‡å®Œä¸€ç§é…æ–™ï¼ˆ$QK^T$ï¼‰ï¼Œå°±æŠŠå®ƒæ”¾å› 500 ç±³å¤–çš„å†°ç®±ï¼ˆHRAMï¼‰ï¼Œè¦ç”¨æ—¶ï¼ˆSoftmaxï¼‰å†è·‘ 500 ç±³å»å–ï¼Œåšå®Œä¸‹ä¸€æ­¥ï¼ˆSoftmax(S)ï¼‰å†æ”¾å›å†°ç®±ï¼Œæœ€åï¼ˆ$OV$ï¼‰å†è·‘ 500 ç±³å»å–... å¨å¸ˆå¤§éƒ¨åˆ†æ—¶é—´éƒ½åœ¨è·‘æ­¥ã€‚
    >   -   FlashAttentionï¼šå¨å¸ˆï¼ˆGPU æ ¸å¿ƒï¼‰æŠŠåš *ä¸€ä»½èœ* æ‰€éœ€çš„æ‰€æœ‰é…æ–™ï¼ˆ$Q, K, V$ çš„ä¸€ä¸ªâ€œå—â€ï¼‰ä¸€æ¬¡æ€§æ‹¿åˆ°è‡ªå·±çš„å°å·¥ä½œå°ï¼ˆSRAMï¼‰ä¸Šã€‚åœ¨å·¥ä½œå°ä¸Šå®Œæˆæ‰€æœ‰æ­¥éª¤ï¼ˆ$S$ å—ã€Softmaxã€ $O$ å—ï¼‰ï¼Œç›´åˆ°åšå‡ºæœ€ç»ˆçš„æˆå“ï¼ˆ$O$ å—ï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œå†æŠŠå®ƒæ”¾åˆ°é¤æ¡Œä¸Šï¼ˆHRAMï¼‰ã€‚å¨å¸ˆå‡ ä¹ä¸éœ€è¦ç¦»å¼€å·¥ä½œå°ã€‚

### 3. å…³é”®å®ç°ï¼šåˆ†å—ä¸åœ¨çº¿ Softmax

è¿™é‡Œæœ€å¤§çš„æŠ€æœ¯éš¾ç‚¹æ˜¯ Softmaxã€‚Softmax éœ€è¦å¯¹ *ä¸€æ•´è¡Œ* æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼ˆ$x_i / \sum(x_j)$ï¼‰ã€‚å¦‚æœæˆ‘åªåŠ è½½äº† $S$ çŸ©é˜µçš„ä¸€ä¸ªå—ï¼ˆ$S_{ij}$ï¼‰ï¼Œæˆ‘æ€ä¹ˆçŸ¥é“è¿™ä¸€è¡Œçš„â€œæœ€å¤§å€¼â€å’Œâ€œæ€»å’Œâ€å‘¢ï¼Ÿ

FlashAttention ä½¿ç”¨äº†ä¸€ç§Online Softmaxçš„æ•°å€¼ç¨³å®šç®—æ³•ï¼š

1.  åˆ†å—è®¡ç®—ï¼š å‡è®¾ $Q$ å—ä¸ $K$ çš„ç¬¬ 1 ä¸ªå— $K_1$ è®¡ç®—ï¼Œå¾—åˆ°äº† $S$ çŸ©é˜µçš„ç¬¬ 1 ä¸ªå— $S_1$ã€‚
2.  è®¡ç®—å±€éƒ¨ Softmaxï¼šåœ¨ SRAM ä¸­è®¡ç®— $S_1$ çš„ Softmaxï¼ˆå¹¶è®°å½•å½“å‰çš„è¡Œæœ€å¤§å€¼ $m_1$ å’Œè¡Œæ€»å’Œ $l_1$ï¼‰ï¼Œç„¶åä¸ $V_1$ ç›¸ä¹˜ï¼Œå¾—åˆ°ä¸€ä¸ªä¸´æ—¶çš„è¾“å‡º $O_1$ã€‚
3.  è¿­ä»£æ›´æ–°ï¼š å½“ $Q$ å—ä¸ $K$ çš„ç¬¬ 2 ä¸ªå— $K_2$ è®¡ç®—ï¼Œå¾—åˆ° $S_2$ã€‚
4.  å¯»æ‰¾æ–°å…¨å±€å€¼ï¼š æ‰¾åˆ° $S_1$ å’Œ $S_2$ å…±åŒçš„è¡Œæœ€å¤§å€¼ $m_{new}$ã€‚
5.  é‡æ–°ç¼©æ”¾ (Rescale)ï¼š
    -   ç¼©æ”¾æ—§å€¼ï¼š å°†ä¹‹å‰è®¡ç®—çš„ $O_1$ æŒ‰ç…§æ–°çš„æœ€å¤§å€¼ $m_{new}$ å’Œæ—§çš„æœ€å¤§å€¼ $m_1$ ä¹‹é—´çš„å·®å€¼è¿›è¡Œç¼©æ”¾ã€‚
    -   è®¡ç®—æ–°å€¼ï¼š æ­£å¸¸è®¡ç®— $S_2$ çš„ Softmaxï¼ˆä½¿ç”¨ $m_{new}$ï¼‰å¹¶ä¸ $V_2$ ç›¸ä¹˜ï¼Œå¾—åˆ° $O_2$ã€‚
6.  ç´¯åŠ ï¼š æœ€ç»ˆçš„è¾“å‡ºå—æ˜¯ $O = (\text{scaled } O_1) + O_2$ã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒFlashAttention å¯ä»¥åœ¨ä¸è®¿é—®å®Œæ•´ $S$ çŸ©é˜µçš„æƒ…å†µä¸‹ï¼Œåˆ†å—è¿­ä»£åœ°è®¡ç®—å‡º**ä¸æ ‡å‡† Softmax å®Œå…¨ç›¸åŒ**çš„ç²¾ç¡®ç»“æœã€‚

æ­¤å¤–ï¼Œåœ¨è®­ç»ƒï¼ˆåå‘ä¼ æ’­ï¼‰æ—¶ï¼Œå®ƒä¸åœ¨å‰å‘ä¼ æ’­ä¸­å­˜å‚¨ $N \times N$ çš„ $S$ çŸ©é˜µï¼Œè€Œæ˜¯åœ¨åå‘ä¼ æ’­ä¸­ä» $Q, K$ **é‡æ–°è®¡ç®—** $S$ çš„å—ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„â€œ**ç”¨è®¡ç®—æ¢å¸¦å®½**â€çš„ç­–ç•¥ï¼Œå› ä¸ºåœ¨ SRAM ä¸­çš„é‡è®¡ç®—ï¼ˆFLOPsï¼‰è¿œæ¯”ä» HRAM è¯»å†™ï¼ˆI/Oï¼‰è¦å¿«ã€‚

### 4. æ ¸å¿ƒä¼˜åŠ¿ (The Advantages)

1.  **æ›´å¿«çš„é€Ÿåº¦ (Faster)ï¼š** FlashAttention å°†æ³¨æ„åŠ›è®¡ç®—ä» I/O é™åˆ¶è½¬å˜ä¸º**è®¡ç®—é™åˆ¶ (FLOPs-bound)**ã€‚ç”±äºå®ƒæå¤§åœ°å‡å°‘äº†å¯¹ HRAM çš„è¯»å†™æ¬¡æ•°ï¼Œå…¶é€Ÿåº¦æ¯”æ ‡å‡†æ³¨æ„åŠ›å¿«å¾—å¤šï¼ˆè®ºæ–‡ä¸­æåˆ°æœ€é«˜ 7.6 å€ï¼ŒFlashAttention-2 ä¸­æ›´å¿«ï¼‰ã€‚
2.  **æ›´ä½çš„æ˜¾å­˜ (Memory Efficient)ï¼š**
    -   **è®­ç»ƒæ—¶ï¼š** **ä¸å†éœ€è¦ç‰©åŒ– $N \times N$ çš„æ³¨æ„åŠ›çŸ©é˜µ**ã€‚è¿™æ˜¯æœ€å¤§çš„èƒœåˆ©ã€‚è¿™ä½¿å¾—æ¨¡å‹å¯ä»¥è®­ç»ƒæ›´é•¿çš„åºåˆ—ï¼ˆLong Contextï¼‰è€Œä¸ä¼š OOM (Out of Memory)ã€‚
    -   **æ¨ç†æ—¶ï¼š** åœ¨å¤„ç†é•¿ Promptï¼ˆPrefill é˜¶æ®µï¼‰æ—¶ï¼ŒFlashAttention åŒæ ·å› ä¸ºä¸äº§ç”Ÿ $N \times N$ çŸ©é˜µè€ŒèŠ‚çœäº†å¤§é‡æ˜¾å­˜å¹¶æå¤§æé€Ÿã€‚
3.  **å®Œå…¨ç­‰ä»· (Numerically Exact)ï¼š** è¿™è‡³å…³é‡è¦ï¼FlashAttention **ä¸æ˜¯ä¸€ä¸ªè¿‘ä¼¼ç®—æ³•**ï¼ˆå¦‚ç¨€ç–æ³¨æ„åŠ›ã€çº¿æ€§æ³¨æ„åŠ›ï¼‰ã€‚å®ƒæ˜¯ä¸€ç§ç¡¬ä»¶æ„ŸçŸ¥çš„ *å®ç°*ï¼Œå…¶è®¡ç®—ç»“æœä¸æ ‡å‡†æ³¨æ„åŠ›åœ¨æ•°å€¼ä¸Šæ˜¯å®Œå…¨ç­‰ä»·çš„ã€‚

æ€»ç»“æ¥è¯´ï¼ŒFlashAttentionï¼ˆåŠå…¶åç»­ç‰ˆæœ¬ FlashAttention-2ï¼Œä»¥åŠFlashAttention-3ï¼‰å·²ç»æˆä¸ºå¤§æ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„äº‹å®æ ‡å‡†ã€‚å®ƒé€šè¿‡ I/O ä¼˜åŒ–çš„æ–¹å¼ï¼Œåœ¨ä¸ç‰ºç‰²æ¨¡å‹ç²¾åº¦çš„å‰æä¸‹ï¼ŒåŒæ—¶è§£å†³äº†æ³¨æ„åŠ›è®¡ç®—çš„é€Ÿåº¦å’Œæ˜¾å­˜ä¸¤å¤§ç“¶é¢ˆã€‚

>   pytorchå†…ç½®çš„scaled_dot_product_attention(SDPA)å‡½æ•°å·²ç»å¸®æˆ‘ä»¬é›†æˆäº†FlashAttentionçš„æ€æƒ³

## 6. çº¿æ€§æ³¨æ„åŠ›ï¼ˆLinear Attentionï¼‰

FlashAttention ä¼˜åŒ–äº† *å¦‚ä½•è®¡ç®—* æ ‡å‡†æ³¨æ„åŠ›ï¼Œä½†æ²¡æœ‰æ”¹å˜å…¶ $O(N^2)$ çš„å¤æ‚åº¦æœ¬è´¨ã€‚å½“åºåˆ—é•¿åº¦ `N` è¾¾åˆ°æ•°åä¸‡ç”šè‡³ä¸Šç™¾ä¸‡æ—¶ï¼Œå³ä½¿æœ‰ FlashAttentionï¼Œè®¡ç®—å’Œå†…å­˜å¼€é”€ä¾ç„¶æ˜¯æ— æ³•æ‰¿å—çš„ã€‚

è€Œ**çº¿æ€§æ³¨æ„åŠ› (Linear Attention)** åˆ™ä»ä¸€ä¸ªæ›´æ¿€è¿›çš„è§’åº¦å‡ºå‘ï¼šå®ƒç›´æ¥**ä¿®æ”¹æ³¨æ„åŠ›çš„æ•°å­¦å…¬å¼**ï¼Œæ—¨åœ¨å°†è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦ä» $O(N^2)$ é™ä½åˆ° $O(N)$ã€‚

è¿™æ˜¯ä¸€ä¸ªæ ¹æœ¬æ€§çš„æ”¹å˜ï¼Œå…¶æ ¸å¿ƒä»£ä»·æ˜¯ï¼š**å®ƒä¸å†æ˜¯ç²¾ç¡®çš„ Softmax æ³¨æ„åŠ›ï¼Œè€Œæ˜¯ä¸€ç§è¿‘ä¼¼**ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„ç”¨**æ¨¡å‹æ€§èƒ½æ¢å–æè‡´æ•ˆç‡**çš„æƒè¡¡ã€‚

### 1. æ ¸å¿ƒæ€æƒ³ï¼šæ”¹å˜è®¡ç®—é¡ºåº

æˆ‘ä»¬å†æ¬¡å›åˆ°æ ‡å‡†æ³¨æ„åŠ›çš„å…¬å¼ï¼š

$O = \text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V$

è¿™ä¸ªå…¬å¼çš„ç“¶é¢ˆåœ¨äº $S = QK^T$ è¿™ä¸ª $N \times N$ çš„çŸ©é˜µã€‚å¦‚æœæˆ‘ä»¬èƒ½é¿å…è®¡ç®—å®ƒï¼Œé—®é¢˜å°±è§£å†³äº†ã€‚

è§‚å¯Ÿè¿™ä¸ªå…¬å¼ï¼Œå¦‚æœæˆ‘ä»¬èƒ½å…ˆè®¡ç®— $K^T V$ï¼Œå†æŠŠå®ƒä¸ $Q$ ç›¸ä¹˜ï¼Œä¼šæ€ä¹ˆæ ·ï¼Ÿ

-   $K^T$ çš„ç»´åº¦æ˜¯ $(d \times N)$ï¼Œ $V$ çš„ç»´åº¦æ˜¯ $(N \times d_v)$ã€‚
-   $K^T V$ çš„ç»“æœæ˜¯ä¸€ä¸ªç»´åº¦ä¸º $(d \times d_v)$ çš„å°çŸ©é˜µï¼Œå…¶å¤§å°ä¸åºåˆ—é•¿åº¦ `N` **æ— å…³**ï¼
-   ç„¶åç”¨ $Q$ (ç»´åº¦ $N \times d$) ä¸è¿™ä¸ª $(d \times d_v)$ çš„çŸ©é˜µç›¸ä¹˜ï¼Œæœ€ç»ˆå¾—åˆ° $(N \times d_v)$ çš„è¾“å‡ºï¼Œæ•´ä¸ªè¿‡ç¨‹çš„å¤æ‚åº¦æ˜¯ $O(N)$ã€‚

**é—®é¢˜åœ¨äºï¼ŒSoftmax å‡½æ•°é˜»æ­¢äº†æˆ‘ä»¬è¿™æ ·åšã€‚** Softmax éœ€è¦ä½œç”¨äº $QK^T$ çš„æ¯ä¸€è¡Œï¼Œå®ƒçš„å­˜åœ¨ä½¿å¾—çŸ©é˜µä¹˜æ³•ä¸æ»¡è¶³ç»“åˆå¾‹ï¼Œæˆ‘ä»¬ä¸èƒ½ç®€å•åœ°æŠŠæ‹¬å·ä» $(QK^T)V$ ç§»åŠ¨åˆ° $Q(K^T V)$ã€‚

çº¿æ€§æ³¨æ„åŠ›çš„æ ¸å¿ƒçªç ´å°±æ˜¯ï¼š**ç”¨ä¸€ä¸ªå¯ä»¥æ»¡è¶³ç»“åˆå¾‹çš„æ ¸å‡½æ•° (Kernel Function) æ¥æ›¿ä»£ Softmax**ã€‚

æˆ‘ä»¬å°†æ³¨æ„åŠ›å…¬å¼ä¸€èˆ¬åŒ–ä¸ºï¼š
$O_i = \sum_{j=1}^{N} \frac{\text{sim}(Q_i, K_j)}{\sum_{k=1}^{N} \text{sim}(Q_i, K_k)} V_j$

å…¶ä¸­ $\text{sim}(Q_i, K_j)$ æ˜¯ $Q_i$ å’Œ $K_j$ çš„ç›¸ä¼¼åº¦å‡½æ•°ã€‚åœ¨æ ‡å‡†æ³¨æ„åŠ›ä¸­ï¼Œ$\text{sim}(Q, K) = \exp(\frac{QK^T}{\sqrt{d_k}})$ã€‚

çº¿æ€§æ³¨æ„åŠ›çš„åšæ³•æ˜¯ï¼Œå°†ç›¸ä¼¼åº¦å‡½æ•° $\text{sim}(Q, K)$ åˆ†è§£ä¸ºä¸¤ä¸ªç‹¬ç«‹ä½œç”¨äº $Q$ å’Œ $K$ çš„å‡½æ•° $\phi(\cdot)$ çš„ç‚¹ç§¯ï¼š
$\text{sim}(Q, K) = \phi(Q)\phi(K)^T$

è¿™æ ·ï¼Œæ³¨æ„åŠ›è®¡ç®—å°±å˜æˆäº†ï¼ˆå¿½ç•¥åˆ†æ¯çš„å½’ä¸€åŒ–é¡¹ï¼‰ï¼š
$O = (\phi(Q)\phi(K)^T)V$

ç”±äºçŸ©é˜µä¹˜æ³•æ»¡è¶³ç»“åˆå¾‹ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥é‡æ–°ç»„åˆå®ƒï¼š
$O = \phi(Q) (\phi(K)^T V)$

è¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼æˆ‘ä»¬æˆåŠŸåœ°å°†è®¡ç®—é¡ºåºæ”¹å˜äº†ï¼š

1.  è®¡ç®— $\phi(K)^T V$ï¼š è¿™æ˜¯ä¸€ä¸ª $(d \times d_v)$ çš„çŸ©é˜µã€‚å¤æ‚åº¦ä¸º $O(N \cdot d \cdot d_v)$ã€‚
2.  è®¡ç®— $\phi(Q)$ï¼š å¤æ‚åº¦ä¸º $O(N \cdot d)$ã€‚
3.  è®¡ç®— $\phi(Q) \times (\phi(K)^T V)$ï¼š å¾—åˆ°æœ€ç»ˆè¾“å‡º $O$ã€‚å¤æ‚åº¦ä¸º $O(N \cdot d \cdot d_v)$ã€‚

æ€»çš„è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦éƒ½å˜æˆäº† $O(N)$ï¼Œä¸åºåˆ—é•¿åº¦ `N` å‘ˆçº¿æ€§å…³ç³»ï¼

### 2. å¦‚ä½•æ›¿æ¢ Softmaxï¼Ÿ

é‚£ä¹ˆï¼Œè¿™ä¸ªç¥å¥‡çš„å‡½æ•° $\phi(\cdot)$ åº”è¯¥æ˜¯ä»€ä¹ˆæ ·å‘¢ï¼Ÿè¿™æ˜¯ä¸åŒçº¿æ€§æ³¨æ„åŠ›å˜ä½“ï¼ˆå¦‚ Performer, Linear Transformer, RWKV ç­‰ï¼‰çš„æ ¸å¿ƒåŒºåˆ«æ‰€åœ¨ã€‚ä¸€ä¸ªç®€å•è€Œå¸¸è§çš„é€‰æ‹©æ˜¯ï¼š

$\phi(x) = \text{elu}(x) + 1$

å…¶ä¸­ `elu` æ˜¯ä¸€ä¸ªæ ‡å‡†çš„æ¿€æ´»å‡½æ•° (Exponential Linear Unit)ã€‚ä½¿ç”¨ `elu + 1` çš„å¥½å¤„æ˜¯ä¿è¯äº†è¾“å‡ºå€¼éè´Ÿï¼Œè¿™åœ¨ä¸€å®šç¨‹åº¦ä¸Šæ¨¡æ‹Ÿäº† Softmax ä¸­ `exp` å‡½æ•°çš„ä½œç”¨ã€‚

å½“ç„¶ï¼Œä¸ºäº†è®©ç»“æœæ›´æ¥è¿‘ Softmaxï¼Œæˆ‘ä»¬è¿˜éœ€è¦åŠ ä¸Šä¹‹å‰å¿½ç•¥çš„å½’ä¸€åŒ–é¡¹ã€‚å®Œæ•´çš„å…¬å¼å˜ä¸ºï¼š
$O_i = \frac{\sum_{j=1}^{N} \phi(Q_i)\phi(K_j)^T V_j}{\sum_{k=1}^{N} \phi(Q_i)\phi(K_k)^T} = \frac{\phi(Q_i) \sum_{j=1}^{N}(\phi(K_j)^T V_j)}{\phi(Q_i) \sum_{k=1}^{N}\phi(K_k)^T}$

åˆ†æ¯å’Œåˆ†å­éƒ½å¯ä»¥åˆ©ç”¨ç»“åˆå¾‹æ¥é«˜æ•ˆè®¡ç®—ã€‚

### 3. çº¿æ€§æ³¨æ„åŠ›çš„ä¼˜åŠ¿ä¸åŠ£åŠ¿

#### ä¼˜åŠ¿ (Advantages)

1.  **çº¿æ€§å¤æ‚åº¦ ($O(N)$):** è¿™æ˜¯æœ€å¤§çš„ä¼˜åŠ¿ã€‚å®ƒä½¿å¾—å¤„ç†æé•¿çš„åºåˆ—ï¼ˆä¾‹å¦‚æ•´æœ¬ä¹¦ã€æ•°å°æ—¶çš„éŸ³é¢‘ï¼‰åœ¨è®¡ç®—ä¸Šæˆä¸ºå¯èƒ½ã€‚
2.  **å¯è¡¨ç¤ºä¸º RNN å½¢å¼:** è¿™æ˜¯çº¿æ€§æ³¨æ„åŠ›ä¸€ä¸ªæå…¶å¼ºå¤§çš„ç‰¹æ€§ã€‚ç”±äºè®¡ç®—å¯ä»¥åˆ†è§£ï¼Œå®ƒå¯ä»¥è¢«å†™æˆä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„å½¢å¼ã€‚
    -   åœ¨æ¨ç†ï¼ˆç”Ÿæˆï¼‰æ—¶ï¼Œæ¯ç”Ÿæˆä¸€ä¸ªæ–° tokenï¼Œæˆ‘ä»¬ä¸éœ€è¦é‡æ–°è®¡ç®—æ•´ä¸ªåºåˆ—çš„æ³¨æ„åŠ›ã€‚
    -   æˆ‘ä»¬åªéœ€è¦ç»´æŠ¤ä¸€ä¸ªå¾ˆå°çš„çŠ¶æ€ï¼ˆå³é‚£ä¸ª $(d \times d_v)$ çš„çŸ©é˜µ $\sum (\phi(K)^T V)$ï¼‰ï¼Œç„¶åç”¨æ–°çš„  $q_{new}$ ä¸ä¹‹äº¤äº’ï¼Œå¹¶ç”¨æ–°çš„ $k_{new}, v_{new}$ æ›´æ–°è¿™ä¸ªçŠ¶æ€ã€‚
    -   è¿™ä½¿å¾— autoregressive æ¨ç†çš„**æ¯ä¸€æ­¥éƒ½æ˜¯ $O(1)$ çš„å¤æ‚åº¦**ï¼Œé€Ÿåº¦æå¿«ï¼Œä¸”ä¸éšå·²ç”Ÿæˆåºåˆ—çš„é•¿åº¦è€Œå˜æ…¢ã€‚è¿™æ­£æ˜¯ RWKV è¿™ç±»æ¶æ„çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚

#### åŠ£åŠ¿ (Disadvantages)

1.  **æ€§èƒ½æŸå¤± (Performance Degradation):** è¿™æ˜¯æœ€ä¸»è¦çš„ä»£ä»·ã€‚çº¿æ€§æ³¨æ„åŠ›çš„æ ¸å‡½æ•° $\phi$ ç»ˆç©¶åªæ˜¯å¯¹ Softmax çš„ä¸€ç§è¿‘ä¼¼ã€‚Softmax çš„æŒ‡æ•°ç‰¹æ€§ä½¿å…¶èƒ½å¤Ÿâ€œèšç„¦â€åˆ°éå¸¸å°‘æ•°çš„å…³é”® token ä¸Šï¼ˆå³æ³¨æ„åŠ›å¾—åˆ†éå¸¸å°–é”ï¼‰ï¼Œè€Œçº¿æ€§æ³¨æ„åŠ›çš„å¤šé¡¹å¼æˆ–ç±»çº¿æ€§æ ¸å‡½æ•°åˆ™æ›´â€œå¹³æ»‘â€ï¼Œéš¾ä»¥å®ç°è¿™ç§å°–é”çš„èšç„¦èƒ½åŠ›ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´åœ¨æ ‡å‡†è¯­è¨€æ¨¡å‹åŸºå‡†æµ‹è¯•ï¼ˆå¦‚å›°æƒ‘åº¦ PPLï¼‰ä¸Šè¡¨ç°ä¸å¦‚æ ‡å‡†æ³¨æ„åŠ›ã€‚
2.  **è¡¨è¾¾èƒ½åŠ›å—é™ (Limited Expressive Power):** ç†è®ºå’Œå®è·µéƒ½è¡¨æ˜ï¼Œçº¿æ€§æ³¨æ„åŠ›çš„è¡¨è¾¾èƒ½åŠ›å¼±äºæ ‡å‡†æ³¨æ„åŠ›ï¼Œå¯¹äºéœ€è¦ç²¾ç¡®æ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»ä¸­ç‰¹å®šâ€œç‚¹å¯¹ç‚¹â€å…³ç³»çš„ä»»åŠ¡ï¼Œå¯èƒ½ä¼šåŠ›ä¸ä»å¿ƒã€‚
3.  **è®­ç»ƒç¨³å®šæ€§ (Training Stability):** æŸäº›çº¿æ€§æ³¨æ„åŠ›çš„å˜ä½“å¯èƒ½åœ¨è®­ç»ƒæ—¶ä¸å¦‚æ ‡å‡†æ³¨æ„åŠ›ç¨³å®šã€‚

## 7. ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰

å‰é¢æˆ‘ä»¬çœ‹åˆ°ï¼ŒFlashAttention ä¼˜åŒ–äº†æ ‡å‡†æ³¨æ„åŠ›çš„è®¡ç®—è¿‡ç¨‹ï¼Œè€Œçº¿æ€§æ³¨æ„åŠ›åˆ™ä¿®æ”¹äº†æ³¨æ„åŠ›çš„æ•°å­¦å…¬å¼ã€‚ç¨€ç–æ³¨æ„åŠ›ï¼ˆSparse Attentionï¼‰èµ°äº†ç¬¬ä¸‰æ¡è·¯ï¼šå®ƒä¿ç•™äº† Softmax æ³¨æ„åŠ›çš„æ ¸å¿ƒå½¢å¼ï¼Œä½†åŸºäºä¸€ä¸ªå…³é”®å‡è®¾æ¥å‡å°‘è®¡ç®—é‡ï¼š**å¤§éƒ¨åˆ†çš„æ³¨æ„åŠ›å¾—åˆ†éƒ½å¾ˆä½ï¼Œæ˜¯æ²¡å¿…è¦è®¡ç®—çš„**ã€‚

ç¨€ç–æ³¨æ„åŠ›çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼š**ä¸å…¶è®©æ¯ä¸ª token å…³æ³¨ï¼ˆAttend toï¼‰åºåˆ—ä¸­çš„æ‰€æœ‰å…¶ä»– tokenï¼ˆå³ç¨ å¯†è¿æ¥ï¼‰ï¼Œä¸å¦‚åªè®©å®ƒå…³æ³¨ä¸€ä¸ªé¢„å…ˆå®šä¹‰å¥½çš„ã€ç¨€ç–çš„å­é›†ã€‚**

è¿™æ˜¯ä¸€ç§ä» $O(N^2)$ çš„â€œAll-to-Allâ€äº¤äº’ï¼Œåˆ° $O(N \log N)$ æˆ– $O(N \sqrt{N})$ çš„â€œSome-to-Someâ€äº¤äº’çš„è½¬å˜ã€‚

### 1. æ ¸å¿ƒç“¶é¢ˆä¸åŠ¨æœº (The Motivation)

åœ¨æ ‡å‡†çš„è‡ªæ³¨æ„åŠ›ä¸­ï¼Œä¸€ä¸ªé•¿åº¦ä¸º `N` çš„åºåˆ—ä¼šäº§ç”Ÿä¸€ä¸ª $N \times N$ çš„æ³¨æ„åŠ›çŸ©é˜µã€‚ç„¶è€Œï¼Œå¤§é‡ç ”ç©¶å’Œå®è·µå‘ç°ï¼Œè¿™ä¸ªçŸ©é˜µé€šå¸¸æ˜¯**ç¨€ç–**çš„ã€‚è¿™æ„å‘³ç€ï¼Œå¯¹äºä¸€ä¸ªç»™å®šçš„ tokenï¼Œå®ƒçœŸæ­£æœ‰æ„ä¹‰çš„æ³¨æ„åŠ›æƒé‡å¾€å¾€åªé›†ä¸­åœ¨å°‘æ•°å‡ ä¸ªå…¶ä»–çš„ token ä¸Šã€‚

ä¾‹å¦‚ï¼Œåœ¨å¥å­ "The quick brown fox jumps over the lazy dog" ä¸­ï¼Œ"jumps" è¿™ä¸ªè¯å¯èƒ½ä¸»è¦å…³æ³¨ "fox" å’Œ "over"ï¼Œè€Œä¸å¥å­æœ«å°¾çš„ "dog" å…³ç³»ä¸å¤§ã€‚è®¡ç®— "jumps" å’Œ "dog" ä¹‹é—´çš„ç²¾ç¡®æ³¨æ„åŠ›å¾—åˆ†ï¼Œå¯èƒ½æ˜¯ä¸€ç§è®¡ç®—èµ„æºçš„æµªè´¹ã€‚

ç¨€ç–æ³¨æ„åŠ›çš„åŠ¨æœºå°±æ˜¯ï¼š**æ—¢ç„¶æœ€ç»ˆçš„æ³¨æ„åŠ›çŸ©é˜µæ˜¯ç¨€ç–çš„ï¼Œæˆ‘ä»¬èƒ½ä¸èƒ½åœ¨è®¡ç®—ä¹‹å‰å°±â€œè·³è¿‡â€é‚£äº›ä¸é‡è¦çš„ token å¯¹ï¼Œä»è€Œé¿å… $O(N^2)$ çš„è®¡ç®—ï¼Ÿ**

### 2. å®ç°ç¨€ç–æ€§çš„å…³é”®ï¼šæ³¨æ„åŠ›æ¨¡å¼ (Sparsity Patterns)

å¦‚ä½•å†³å®šå“ªäº› token å¯¹æ˜¯â€œé‡è¦çš„â€å¹¶éœ€è¦è®¡ç®—å‘¢ï¼Ÿè¿™æ˜¯å„ç§ç¨€ç–æ³¨æ„åŠ›æ¨¡å‹ï¼ˆå¦‚ Longformerã€BigBird ç­‰ï¼‰åˆ›æ–°çš„æ ¸å¿ƒã€‚å®ƒä»¬è®¾è®¡äº†ä¸åŒçš„**å›ºå®šç¨€ç–æ¨¡å¼ (Fixed Sparsity Patterns)** æ¥è¿‘ä¼¼å®Œæ•´çš„æ³¨æ„åŠ›ã€‚

æœ€å¸¸è§çš„å‡ ç§æ¨¡å¼åŒ…æ‹¬ï¼š

#### a) æ»‘åŠ¨çª—å£æ³¨æ„åŠ› (Sliding Window / Local Attention)

-   **æ€æƒ³**ï¼šä¸€ä¸ª token ä¸»è¦ä¸å…¶é‚»è¿‘çš„ token ç›¸å…³ã€‚è¿™åœ¨è¯­è¨€å’Œè§†è§‰ä¸­éƒ½æ˜¯ä¸€ä¸ªéå¸¸å¼ºçš„å…ˆéªŒçŸ¥è¯†ã€‚
-   **å®ç°**ï¼šæ¯ä¸ª token åªå…³æ³¨å…¶å·¦è¾¹å’Œå³è¾¹ `w` ä¸ª tokenï¼ˆ`w` æ˜¯çª—å£å¤§å°ï¼‰ã€‚
-   **å¤æ‚åº¦**ï¼šæ¯ä¸ª token åªè®¡ç®— `2w` ä¸ªå¾—åˆ†ï¼Œæ€»å¤æ‚åº¦ä¸º $O(N \cdot w)$ã€‚å¦‚æœ `w` æ˜¯ä¸€ä¸ªå°çš„å¸¸æ•°ï¼Œå¤æ‚åº¦å°±æ˜¯çº¿æ€§çš„ $O(N)$ã€‚
-   **é—®é¢˜**ï¼šè¿™ç§æ¨¡å¼å®Œå…¨åˆ‡æ–­äº†è¿œè·ç¦»çš„ä¾èµ–å…³ç³»ã€‚çª—å£ä¹‹å¤–çš„ token ä¹‹é—´æ— æ³•ç›´æ¥äº¤æ¢ä¿¡æ¯ã€‚

#### b) æ‰©å¼ /ç©ºæ´æ»‘åŠ¨çª—å£ (Dilated / Strided Sliding Window)

-   **æ€æƒ³**ï¼šä¸ºäº†åœ¨ä¸å¢åŠ è®¡ç®—é‡çš„æƒ…å†µä¸‹æ‰©å¤§æ„Ÿå—é‡ï¼ˆReceptive Fieldï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥è®©çª—å£å†…å­˜åœ¨â€œé—´éš™â€ã€‚
-   **å®ç°**ï¼šçª—å£å†…çš„ token ä¸æ˜¯è¿ç»­çš„ï¼Œè€Œæ˜¯ä»¥ä¸€å®šçš„æ­¥é•¿ï¼ˆdilation rateï¼‰è·³è·ƒå¼é€‰æ‹©ã€‚ä¾‹å¦‚ï¼Œå…³æ³¨ä½ç½® `i-4, i-2, i, i+2, i+4`ã€‚
-   **ä¼˜åŠ¿**ï¼šé€šè¿‡åœ¨ä¸åŒå±‚æˆ–ä¸åŒå¤´ä½¿ç”¨ä¸åŒçš„æ‰©å¼ ç‡ï¼Œæ¨¡å‹å¯ä»¥æ•æ‰åˆ°ä¸åŒå°ºåº¦çš„è¿œè·ç¦»ä¾èµ–ã€‚

#### c) å…¨å±€æ³¨æ„åŠ› (Global Attention)

-   **æ€æƒ³**ï¼šåºåˆ—ä¸­æœ‰ä¸€äº›ç‰¹æ®Šçš„â€œæ˜æ˜Ÿâ€ tokenï¼Œå®ƒä»¬åº”è¯¥æœ‰èƒ½åŠ›å…³æ³¨æ‰€æœ‰å…¶ä»– tokenï¼Œä¹Ÿåº”è¯¥è¢«æ‰€æœ‰å…¶ä»– token å…³æ³¨ã€‚
-   **å®ç°**ï¼šé¢„å…ˆé€‰æ‹©å°‘æ•°å‡ ä¸ª token ä½œä¸ºå…¨å±€ tokenï¼ˆä¾‹å¦‚ï¼Œ`[CLS]` tokenï¼‰ã€‚è¿™äº› token çš„æ³¨æ„åŠ›è®¡ç®—æ˜¯ç¨ å¯†çš„ï¼ˆ$O(N)$ï¼‰ï¼Œä½†ç”±äºæ•°é‡å¾ˆå°‘ï¼Œæ€»çš„é¢å¤–å¼€é”€ä¸å¤§ã€‚
-   **ä¼˜åŠ¿**ï¼šè¿™æ˜¯è§£å†³æ»‘åŠ¨çª—å£æ— æ³•ä¼ é€’é•¿è·ç¦»ä¿¡æ¯çš„å…³é”®ã€‚ä¿¡æ¯å¯ä»¥é€šè¿‡å…¨å±€ token åœ¨åºåˆ—çš„ä¸åŒéƒ¨åˆ†ä¹‹é—´ä¼ é€’ã€‚

#### d) éšæœºæ³¨æ„åŠ› (Random Attention)

-   **æ€æƒ³**ï¼šä¸ºäº†å¼¥è¡¥å›ºå®šæ¨¡å¼å¯èƒ½é”™è¿‡çš„è¿æ¥ï¼Œä¸ºæ¯ä¸ª token é¢å¤–å¢åŠ å‡ ä¸ªéšæœºé€‰æ‹©çš„ token è¿›è¡Œå…³æ³¨ã€‚
-   **å®ç°**ï¼šæ¯ä¸ª token é™¤äº†å…³æ³¨å…¶å›ºå®šæ¨¡å¼å†…çš„ token å¤–ï¼Œè¿˜éšæœºé‡‡æ · `r` ä¸ªåºåˆ—ä¸­çš„å…¶ä»– tokenã€‚
-   **ä¼˜åŠ¿**ï¼šå¢åŠ äº†è¿æ¥çš„é²æ£’æ€§ï¼Œç†è®ºä¸Šä¿è¯äº†ä¿¡æ¯å¯ä»¥åœ¨åºåˆ—çš„ä»»æ„ä¸¤ç‚¹é—´ä»¥è¾ƒçŸ­çš„è·¯å¾„ä¼ æ’­ã€‚

**ç»„åˆæ¨¡å¼ (Combined Patterns)**
æœ€æˆåŠŸçš„ç¨€ç–æ³¨æ„åŠ›æ¨¡å‹ï¼Œå¦‚ **Longformer** å’Œ **BigBird**ï¼Œé€šå¸¸ä¼š**ç»„åˆ**ä¸Šè¿°å¤šç§æ¨¡å¼ã€‚ä¾‹å¦‚ï¼ŒLongformer å°±ç»“åˆäº†**æ»‘åŠ¨çª—å£æ³¨æ„åŠ›**å’Œ**å…¨å±€æ³¨æ„åŠ›**ï¼Œä½¿å¾—æ¨¡å‹æ—¢èƒ½é«˜æ•ˆå¤„ç†å±€éƒ¨ä¿¡æ¯ï¼Œåˆèƒ½é€šè¿‡å…¨å±€ token æ•æ‰é•¿è·ç¦»ä¾èµ–ã€‚

### 3. ä¼˜åŠ¿ä¸åŠ£åŠ¿

#### ä¼˜åŠ¿ (Advantages)

1.  **æ›´é«˜çš„è®¡ç®—æ•ˆç‡**ï¼šå°†å¤æ‚åº¦ä» $O(N^2)$ é™ä½åˆ° $O(N \log N)$ æˆ– $O(N\sqrt{N})$ï¼Œæ˜¾è‘—å‡å°‘äº†è®¡ç®—é‡å’Œå†…å­˜éœ€æ±‚ã€‚
2.  **æ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡**ï¼šè¿™æ˜¯æœ€ç›´æ¥çš„å¥½å¤„ã€‚åƒ Longformer è¿™æ ·çš„æ¨¡å‹å¯ä»¥å°† Transformer çš„ä¸Šä¸‹æ–‡é•¿åº¦ä» 512 æˆ– 1024 æ‰©å±•åˆ° 4096 ç”šè‡³æ›´é•¿ã€‚
3.  **ä¿ç•™äº†å¼ºå¤§çš„è¡¨è¾¾èƒ½åŠ›**ï¼šä¸çº¿æ€§æ³¨æ„åŠ›ç›¸æ¯”ï¼Œç¨€ç–æ³¨æ„åŠ›ä»ç„¶ä½¿ç”¨ Softmaxï¼Œä¿ç•™äº†å…¶â€œèšç„¦â€èƒ½åŠ›ï¼Œå› æ­¤åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šæ¯”çº¿æ€§æ³¨æ„åŠ›çš„æ€§èƒ½æŸå¤±æ›´å°ã€‚

#### åŠ£åŠ¿ (Disadvantages)

1.  **æ˜¯è¿‘ä¼¼ï¼Œéç²¾ç¡®**ï¼šè¿™æ˜¯ä¸ FlashAttention æœ€å¤§çš„ä¸åŒã€‚ç¨€ç–æ³¨æ„åŠ›æ˜¯ä¸€ç§**è¿‘ä¼¼**ï¼Œå®ƒåŸºäºâ€œå¤§å¤šæ•°è¿æ¥ä¸é‡è¦â€çš„å‡è®¾ã€‚å¦‚æœä»»åŠ¡ä¸­å­˜åœ¨ä¸€ä¸ªå…³é”®çš„ã€ä½†ä¸ç¬¦åˆé¢„è®¾ç¨€ç–æ¨¡å¼çš„é•¿è·ç¦»ä¾èµ–ï¼Œæ¨¡å‹å°±å¯èƒ½æ•æ‰ä¸åˆ°ã€‚
2.  **å®ç°å¤æ‚**ï¼šä¸æ ‡å‡†çš„ç¨ å¯†çŸ©é˜µä¹˜æ³•ä¸åŒï¼Œç¨€ç–æ³¨æ„åŠ›éœ€è¦ä¸“é—¨çš„ CUDA Kernel æ¥é«˜æ•ˆå®ç°ã€‚å¦‚æœä¸è¿›è¡Œåº•å±‚ä¼˜åŒ–ï¼Œåœ¨ GPU ä¸Šå¯¹ç¨€ç–çŸ©é˜µè¿›è¡Œæ“ä½œï¼ˆéœ€è¦å¤„ç†å„ç§ç´¢å¼•ï¼‰å¯èƒ½æ¯”ç›´æ¥è®¡ç®—ç¨ å¯†çŸ©é˜µè¿˜è¦æ…¢ã€‚
3.  **æ¨¡å¼ä¾èµ–**ï¼šæ¨¡å‹çš„æ€§èƒ½å¯èƒ½ä¾èµ–äºæ‰€é€‰æ‹©çš„ç¨€ç–æ¨¡å¼ï¼Œéœ€è¦æ ¹æ®ä»»åŠ¡è¿›è¡Œè°ƒæ•´ï¼Œç¼ºä¹é€šç”¨æ€§ã€‚

>   è¦çœŸæ­£ä»ç¨€ç–æ³¨æ„åŠ›ä¸­è·ç›Šï¼Œå…³é”®åœ¨äºå…¶å®ç°æ–¹å¼ã€‚å…¶æ ¸å¿ƒåŸåˆ™æ˜¯ä»ä¸€å¼€å§‹å°±é¿å…è®¡ç®—å’Œå­˜å‚¨å®Œæ•´çš„ $N \times N$ æ³¨æ„åŠ›çŸ©é˜µã€‚ä»¥ä¸‹æ˜¯å®ç°è¿™ä¸€ç›®æ ‡çš„ä¸»æµæ€è·¯ï¼š
>
>   ä»»ä½•å…ˆè®¡ç®—å‡ºç¨ å¯†æ³¨æ„åŠ›çŸ©é˜µï¼Œå†ç”¨æ©ç ä½¿å…¶ç¨€ç–çš„æ–¹æ³•ï¼Œéƒ½æ— æ³•é™ä½ $O(N^2)$ çš„è®¡ç®—å¤æ‚åº¦ï¼Œå› æ­¤æ˜¯æ— æ•ˆçš„ã€‚é«˜æ•ˆçš„å®ç°å¿…é¡»åœ¨è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ä¹‹å‰ï¼Œå°±åªå¤„ç†é¢„è®¾æ¨¡å¼ä¸­çš„ (Query, Key) å¯¹ã€‚
>
>   **æ–¹æ³•ä¸€ï¼šæ•°æ®é‡æ’ä¸æ‰¹å¤„ç†çŸ©é˜µä¹˜æ³• (æ¡†æ¶çº§å®ç°)**
>
>   è¿™æ˜¯ä¸€ç§åœ¨ PyTorch æˆ– TensorFlow ç­‰é«˜çº§æ¡†æ¶ä¸­ä¸éœ€ç¼–å†™åº•å±‚ä»£ç å³å¯å®ç°çš„æ–¹æ³•ï¼Œæ€è·¯å¦‚ä¸‹ï¼š
>
>   1.  ç¡®å®šç´¢å¼•ï¼šå¯¹äºåºåˆ—ä¸­çš„æ¯ä¸€ä¸ª Query $q_i$ï¼Œæ ¹æ®ç¨€ç–æ¨¡å¼ï¼ˆå¦‚æ»‘åŠ¨çª—å£ï¼‰è®¡ç®—å‡ºå®ƒéœ€è¦äº¤äº’çš„æ‰€æœ‰ Key $k_j$ çš„ç´¢å¼•ã€‚
>   2.  æ”¶é›†æ•°æ® (Gather)ï¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ã€æ›´å°çš„ Key çŸ©é˜µå’Œ Value çŸ©é˜µã€‚ä¾‹å¦‚ï¼Œå¯¹äºæ»‘åŠ¨çª—å£æ³¨æ„åŠ›ï¼Œè¿™ä¸ªæ–°çŸ©é˜µçš„ç»´åº¦ä¼šæ˜¯ `(N, window_size, d)`ã€‚è¿™ä¸€æ­¥é€šè¿‡é«˜æ•ˆçš„ç´¢å¼•å’Œæ•°æ®å¤åˆ¶æ“ä½œï¼ˆå¦‚ `torch.gather` æˆ– `unfold`ï¼‰å®Œæˆã€‚
>   3.  æ‰¹å¤„ç†è®¡ç®— (Batched Compute)ï¼šå°†åŸå§‹çš„ `Q` çŸ©é˜µä¸è¿™ä¸ªæ–°çš„ã€å°å‹çš„ `K_windowed` çŸ©é˜µè¿›è¡Œæ‰¹å¤„ç†çŸ©é˜µä¹˜æ³• (Batched Matrix Multiplication)ã€‚è¿™ä¼šå°† $N$ ä¸ªç‹¬ç«‹çš„ã€å°è§„æ¨¡çš„æ³¨æ„åŠ›è®¡ç®—å¹¶è¡ŒåŒ–ï¼Œæ€»çš„è®¡ç®—é‡æ˜¯ $O(N \cdot \text{window\_size})$ï¼Œè€Œé $O(N^2)$ã€‚
>
>   è¿™ç§æ–¹æ³•è™½ç„¶ä¼šäº§ç”Ÿä¸€ä¸ªä¸­é—´çš„ã€é‡æ’åçš„ `K_windowed` çŸ©é˜µï¼Œä½†å·²ç»æˆåŠŸåœ°å°†è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦ä»äºŒæ¬¡æ–¹é™ä½åˆ°äº†è¿‘ä¼¼çº¿æ€§ã€‚
>
>   ##### æ–¹æ³•äºŒï¼šèåˆçš„è‡ªå®šä¹‰ CUDA Kernel (æ€§èƒ½æè‡´åŒ–)
>
>   è¿™æ˜¯åƒ Longformer å’Œ FlashAttention ç­‰ SOTA å®ç°æ‰€é‡‡ç”¨çš„æ–¹æ³•ï¼Œè¿½æ±‚æè‡´çš„ç¡¬ä»¶æ•ˆç‡ï¼š
>
>   1.  æ ¸å‡½æ•°èåˆ (Kernel Fusion)ï¼šå°†â€œç¡®å®šç´¢å¼•â€ã€â€œæ”¶é›†æ•°æ®â€å’Œâ€œè®¡ç®—å¾—åˆ†â€ç­‰å¤šä¸ªæ­¥éª¤èåˆåˆ°ä¸€ä¸ªå•ç‹¬çš„ GPU æ ¸å‡½æ•°ä¸­ã€‚
>   2.  I/O æ„ŸçŸ¥è®¡ç®—ï¼šè¯¥æ ¸å‡½æ•°ç›´æ¥åœ¨ GPU ä¸Šè¿è¡Œã€‚å¯¹äºæ¯ä¸€ä¸ª Query $q_i$ï¼š
>       -   å°†å…¶åŠ è½½åˆ° GPU æ ¸å¿ƒçš„é«˜é€Ÿç¼“å­˜ SRAM ä¸­ã€‚
>       -   æ ¹æ®ç¨€ç–æ¨¡å¼ï¼Œç›´æ¥ä»æ…¢é€Ÿçš„ HRAM (æ˜¾å­˜) ä¸­æŒ‰éœ€è¯»å–å¯¹åº”çš„ $k_j$ å’Œ $v_j$ åˆ° SRAMã€‚
>       -   æ‰€æœ‰è®¡ç®—ï¼ˆç‚¹ç§¯ã€Softmaxã€ä¸ Value ç›¸ä¹˜ï¼‰éƒ½åœ¨é«˜é€Ÿçš„ SRAM å†…éƒ¨å®Œæˆã€‚
>       -   åªå°†æœ€ç»ˆçš„è¾“å‡º $o_i$ å†™å›åˆ° HRAMã€‚
>
>   è¿™ç§æ–¹æ³•å€Ÿé‰´äº† FlashAttention çš„ I/O æ„ŸçŸ¥æ€æƒ³ï¼Œå®Œå…¨é¿å…äº†åœ¨ HRAM ä¸­åˆ›å»ºä»»ä½•ä¸­é—´çŸ©é˜µï¼ˆå¦‚ `K_windowed`ï¼‰ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†å¯¹å†…å­˜å¸¦å®½çš„å ç”¨ï¼Œæ˜¯å½“å‰å®ç°ç¨€ç–æ³¨æ„åŠ›çš„æœ€é«˜æ•ˆæ–¹å¼ã€‚

æˆ‘ä»¬ç°åœ¨æœ‰äº†å››ç§ä¸»è¦çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œå¯ä»¥ç”¨ä¸€å¼ è¡¨æ ¼æ¥æ¸…æ™°åœ°å¯¹æ¯”å®ƒä»¬ï¼š

| ç‰¹æ€§     | æ ‡å‡† Attention                 | FlashAttention                        | çº¿æ€§ Attention                     | ç¨€ç– Attention                 |
| :------- | :----------------------------- | :------------------------------------ | :--------------------------------- | :----------------------------- |
| æ ¸å¿ƒæ€æƒ³ | ç¨ å¯†çš„ All-to-All              | I/O æ„ŸçŸ¥çš„å·¥ç¨‹ä¼˜åŒ–                    | ä¿®æ”¹æ•°å­¦å…¬å¼è¿‘ä¼¼ Softmax           | é¢„è®¾æ¨¡å¼è·³è¿‡éƒ¨åˆ†è®¡ç®—           |
| ç­‰ä»·æ€§   | -                              | **å®Œå…¨ç­‰ä»·**                          | **ä¸ç­‰ä»·** (è¿‘ä¼¼)                  | **ä¸ç­‰ä»·** (è¿‘ä¼¼)              |
| å¤æ‚åº¦   | $O(N^2)$                       | $O(N^2)$                              | $O(N)$                             | $O(N\log N)$ æˆ– $O(N\sqrt{N})$ |
| ä¼˜åŠ¿     | è¡¨è¾¾èƒ½åŠ›å¼ºï¼Œæ˜¯é»„é‡‘æ ‡å‡†         | é€Ÿåº¦å¿«ï¼Œæ˜¾å­˜ä½ï¼Œæ— æŸç²¾åº¦              | æè‡´æ•ˆç‡ï¼Œæ¨ç†$O(1)$ï¼Œæ”¯æŒè¶…é•¿åºåˆ— | å¹³è¡¡æ•ˆç‡ä¸æ€§èƒ½ï¼Œæ”¯æŒé•¿åºåˆ—     |
| åŠ£åŠ¿     | é€Ÿåº¦æ…¢ï¼Œæ˜¾å­˜å ç”¨é«˜             | å¤æ‚åº¦æœ¬è´¨æœªå˜                        | æŸå¤±æ¨¡å‹æ€§èƒ½                       | å®ç°å¤æ‚ï¼Œå¯èƒ½ä¸¢å¤±å…³é”®ä¿¡æ¯     |
| ä»£è¡¨åº”ç”¨ | æ—©æœŸ Transformer (BERT, GPT-2) | **å½“å‰æ‰€æœ‰ä¸»æµå¤§æ¨¡å‹ (Llama, GPT-4)** | RWKV, Performer                    | Longformer, BigBird            |

ç¨€ç–æ³¨æ„åŠ›æ˜¯è§£å†³ Transformer é•¿åºåˆ—é—®é¢˜çš„é‡Œç¨‹ç¢‘å¼å·¥ä½œã€‚å®ƒåœ¨ç†è®ºä¸Šè¯æ˜äº†æˆ‘ä»¬å¯ä»¥é€šè¿‡èªæ˜çš„è¿‘ä¼¼æ¥å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦ï¼Œå‚¬ç”Ÿäº†ä¸€ç³»åˆ—æˆåŠŸçš„é•¿ä¸Šä¸‹æ–‡æ¨¡å‹ã€‚

ç„¶è€Œï¼Œéšç€ **FlashAttention** çš„å‡ºç°ï¼Œæƒ…å†µå‘ç”Ÿäº†å˜åŒ–ã€‚FlashAttention æå¤§åœ°ä¼˜åŒ–äº†**ç²¾ç¡®æ³¨æ„åŠ›**çš„è®¡ç®—æ•ˆç‡ï¼Œä½¿å¾—åœ¨å½“å‰ç¡¬ä»¶ï¼ˆå¦‚ A100/H100ï¼‰ä¸Šï¼Œå¤„ç†ä¸­ç­‰é•¿åº¦ï¼ˆå¦‚ 4K-16Kï¼‰çš„**ç¨ å¯†æ³¨æ„åŠ›**å˜å¾—éå¸¸å¿«ã€‚è¿™ä½¿å¾—ç¨€ç–æ³¨æ„åŠ›çš„å·¥ç¨‹ä¼˜åŠ¿åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¢«å‰Šå¼±äº†ã€‚ä¸è¿‡ï¼Œç¨€ç–åŒ–çš„æ€æƒ³ä»ç„¶æå…·ä»·å€¼ï¼Œå°¤å…¶æ˜¯åœ¨æœªæ¥éœ€è¦å¤„ç†æ•°åä¸‡ç”šè‡³ä¸Šç™¾ä¸‡é•¿åº¦åºåˆ—çš„åœºæ™¯ä¸‹ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦çš„é‡çº§ä»ç„¶æ˜¯ä¸å¯æˆ–ç¼ºçš„ã€‚